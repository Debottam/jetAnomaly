{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality Sensitive Hashing To separate Signal and Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import h5py\n",
    "import pickle\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "#import deepdish.io as io\n",
    "import tensorflow as tf\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import scale, normalize\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_random_state\n",
    "from collections import Counter\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading training and testing sample\n",
    "\n",
    "Already passed on a trained encoder of a VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_encoded_train=h5py.File('/lcg/storage13/atlas/gupta/encoded_x_train_mid.h5','r')\n",
    "f_encoded_test = h5py.File('/lcg/storage13/atlas/gupta/encoded_x_test_mid.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_x_train_mid=f_encoded_train['table']\n",
    "encoded_x_test_mid=f_encoded_test['table']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As VAE-encoded data has mu, sigma and mu+sigma we use only use mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 3\n",
    "encoded_x_train = encoded_x_train_mid[-level]\n",
    "encoded_x_test = encoded_x_test_mid[-level]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating hash codes for each training/bkg data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493900"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_size = 10\n",
    "sample_size, vec_size = encoded_x_train.shape\n",
    "projections = np.random.randn(hash_size, vec_size)\n",
    "hashed_encoded = list()\n",
    "for i in range(sample_size):\n",
    "    bools = ''\n",
    "    for j in range(hash_size):\n",
    "        bool = (np.dot(encoded_x_train[i], projections[j].T) > 0).astype('int')\n",
    "        bools += str(bool)\n",
    "    hashed_encoded.append(bools)\n",
    "len(hashed_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all the hashcodes and their fequency in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDuplicatesWithInfo(hashed_encoded):\n",
    "    ''' Get duplicate element in a list along with thier indices in list\n",
    "     and frequency count'''\n",
    "    dictOfElems = dict()\n",
    "    index = 0\n",
    "    # Iterate over each element in list and keep track of index\n",
    "    for elem in hashed_encoded:\n",
    "        # If element exists in dict then keep its index in lisr & increment its frequency\n",
    "        if elem in dictOfElems:\n",
    "            dictOfElems[elem][0] += 1\n",
    "            dictOfElems[elem][1].append(index)\n",
    "        else:\n",
    "            # Add a new entry in dictionary \n",
    "            dictOfElems[elem] = [1, [index]]\n",
    "        index += 1    \n",
    " \n",
    "    dictOfElems = { key:value for key, value in dictOfElems.items() }\n",
    "    return dictOfElems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictOfElems = getDuplicatesWithInfo(hashed_encoded)\n",
    "listHash = list()\n",
    "listOther = list()\n",
    "for key, value in dictOfElems.items():\n",
    "    listHash.append(key)\n",
    "    listOther.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lcg/storage15/software64/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "listHash = np.array(listHash)\n",
    "listOther = np.array(listOther)\n",
    "bkg_train_freq = listOther[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_train_freq_ind = listOther[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29334"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bkg_train_freq_ind[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29334"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bkg_train_freq[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the test sample which is signal+bkg \n",
    "\n",
    "If the hashcode of any test data point doesn't belong to listHash (list of hashcodes generated by training data) that datapoint will be marked as novel or signal or anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "tableSize= len(listHash)\n",
    "sample_size, vec_size = encoded_x_test.shape\n",
    "hash_truth = list()\n",
    "bkgCount = list()\n",
    "count = 0\n",
    "for i in range(sample_size):\n",
    "    bools = ''\n",
    "    a = 1\n",
    "    for j in range(hash_size):\n",
    "        bool = (np.dot(encoded_x_test[i], projections[j].T) > 0).astype('int')\n",
    "        bools += str(bool)\n",
    "    for k in range(tableSize):\n",
    "        if bools == listHash[k]:\n",
    "            bkgCount.append(k)\n",
    "            a=0\n",
    "            break\n",
    "    if(a==1):\n",
    "        count+=1\n",
    "    hash_truth.append(a)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_3 = '/lcg/storage13/atlas/gupta/hash_truth'+'str(level)'+'str(hash_size)'+'.h5'\n",
    "h5f_hash_truth = h5py.File(filename_3, 'w')\n",
    "h5f_hash_truth.create_dataset('table', data=hash_truth, compression=\"gzip\")\n",
    "h5f_hash_truth.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_encoded_train.close()\n",
    "f_encoded_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
